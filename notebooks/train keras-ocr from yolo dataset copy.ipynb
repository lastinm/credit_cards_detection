{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f228ec",
   "metadata": {},
   "source": [
    "Чтобы использовать датасет, подготовленный для YOLO, для обучения keras-ocr, вам нужно преобразовать данные в формат, который понимает keras-ocr.\n",
    "\n",
    "1. Разница между форматами YOLO и Keras-OCR\n",
    "YOLO формат обычно хранит аннотации в .txt-файлах с координатами bounding box в формате:\n",
    "\n",
    "Copy\n",
    "class_id x_center y_center width height\n",
    "(нормализованные относительно ширины и высоты изображения).\n",
    "\n",
    "Keras-OCR ожидает данные в виде списка пар (image, [текст1, текст2, ...]), где bounding boxes могут быть в формате [[x1, y1], [x2, y2], [x3, y3], [x4, y4]] (координаты углов полигона).\n",
    "\n",
    "2. Преобразование YOLO → Keras-OCR\n",
    "\n",
    "# Шаг 1. Загрузка и парсинг YOLO-аннотаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58522fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_ocr\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fadc9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_yolo_annotation(img_path, label_path):\n",
    "    # Загружаем изображение, чтобы узнать его размеры\n",
    "    img = cv2.imread(img_path)\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    \n",
    "    # Читаем аннотации из .txt-файла\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, w, h = map(float, line.strip().split())\n",
    "        \n",
    "        # Конвертация из нормализованных координат YOLO в абсолютные\n",
    "        x_center *= img_w\n",
    "        y_center *= img_h\n",
    "        w *= img_w\n",
    "        h *= img_h\n",
    "        \n",
    "        # Вычисление координат углов прямоугольника (x1, y1, x2, y2)\n",
    "        x1 = int(x_center - w / 2)\n",
    "        y1 = int(y_center - h / 2)\n",
    "        x2 = int(x_center + w / 2)\n",
    "        y2 = int(y_center + h / 2)\n",
    "        \n",
    "        # Добавляем в формате полигона (4 угла)\n",
    "        box = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]\n",
    "        boxes.append(box)\n",
    "    \n",
    "    return img, boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebe0a8",
   "metadata": {},
   "source": [
    "# Шаг 2. Создание датасета для Keras-OCR\n",
    "\n",
    "Keras-OCR ожидает данные в виде списка пар (изображение, список текстов). Если ваш датасет содержит только цифры, можно сгенерировать метки автоматически (предполагая, что class_id соответствует цифре):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1228d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_dataset(data_dir):\n",
    "    dataset = []\n",
    "    for img_file in os.listdir(os.path.join(data_dir, \"images\")):\n",
    "        if not img_file.endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(data_dir, \"images\", img_file)\n",
    "        label_path = os.path.join(data_dir, \"labels\", img_file.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        \n",
    "        img, boxes = parse_yolo_annotation(img_path, label_path)\n",
    "        \n",
    "        # Предполагаем, что class_id = цифра (0 → \"0\", 1 → \"1\" и т. д.)\n",
    "        texts = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_id = int(line.strip().split()[0])\n",
    "                texts.append(str(class_id))\n",
    "        \n",
    "        dataset.append((img, texts))  # Keras-OCR ожидает (image, [text1, text2, ...])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d2e84",
   "metadata": {},
   "source": [
    "# Шаг 3. Обучение Keras-OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946a664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "dataset = load_yolo_dataset(\"/home/lastinm/PROJECTS/DATA/syntetic digits\")\n",
    "\n",
    "train_dataset = dataset[:int(0.8 * len(dataset))]\n",
    "val_dataset = dataset[int(0.8 * len(dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c0a2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_to_keras_generator(yolo_dataset, batch_size=8, target_size=(640, 640)):\n",
    "    \"\"\"Генератор с фиксированным количеством аннотаций на изображение\"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(yolo_dataset))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, len(yolo_dataset), batch_size):\n",
    "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "            batch_images = []\n",
    "            batch_texts = []\n",
    "            batch_boxes = []\n",
    "            \n",
    "            for idx in batch_indices:\n",
    "                img, annotations = yolo_dataset[idx]\n",
    "                \n",
    "                # Преобразование изображения\n",
    "                if len(img.shape) == 2:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                else:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                img = cv2.resize(img, target_size)\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "                \n",
    "                # Обработка аннотаций\n",
    "                texts = []\n",
    "                boxes = []\n",
    "                \n",
    "                if isinstance(annotations, str):\n",
    "                    texts.append(annotations)\n",
    "                    boxes.append([[0, 0], [target_size[0], 0], \n",
    "                                [target_size[0], target_size[1]], [0, target_size[1]]])\n",
    "                elif isinstance(annotations, list):\n",
    "                    for ann in annotations[:5]:  # Берем не более 5 аннотаций на изображение\n",
    "                        if isinstance(ann, str):\n",
    "                            texts.append(ann)\n",
    "                            boxes.append([[0, 0], [target_size[0], 0], \n",
    "                                         [target_size[0], target_size[1]], [0, target_size[1]]])\n",
    "                        elif len(ann) >= 2:\n",
    "                            texts.append(str(ann[0]))\n",
    "                            scale_x = target_size[0] / img.shape[1]\n",
    "                            scale_y = target_size[1] / img.shape[0]\n",
    "                            scaled_box = []\n",
    "                            for point in ann[1]:\n",
    "                                scaled_box.append([point[0]*scale_x, point[1]*scale_y])\n",
    "                            boxes.append(scaled_box)\n",
    "                \n",
    "                # Дополняем до 5 аннотаций (если меньше)\n",
    "                while len(texts) < 5:\n",
    "                    texts.append(\"\")  # Пустая строка как padding\n",
    "                    boxes.append([[0, 0], [0, 0], [0, 0], [0, 0]])\n",
    "                \n",
    "                batch_images.append(img)\n",
    "                batch_texts.append(texts[:5])  # Берем ровно 5 текстов\n",
    "                batch_boxes.append(boxes[:5])   # И 5 соответствующих bbox\n",
    "            \n",
    "            yield np.array(batch_images), {\n",
    "                'output_text': np.array(batch_texts),\n",
    "                'output_box': np.array(batch_boxes)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22575011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изображения: (16, 640, 640, 3)\n",
      "Тексты: (16, 5)\n",
      "Боксы: (16, 5, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "train_gen = yolo_to_keras_generator(train_dataset, batch_size=16)\n",
    "val_gen = yolo_to_keras_generator(val_dataset, batch_size=16)\n",
    "images, labels = next(train_gen)\n",
    "\n",
    "print(\"Изображения:\", images.shape)\n",
    "print(\"Тексты:\", labels['output_text'].shape)\n",
    "print(\"Боксы:\", labels['output_box'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c4a27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "batch_size = 16\n",
    "target_size = (640, 640)  # Все изображения будут приведены к этому размеру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "357996df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/lastinm/.keras-ocr/craft_mlt_25k.h5\n",
      "Looking for /home/lastinm/.keras-ocr/crnn_kurapan.h5\n"
     ]
    }
   ],
   "source": [
    "# Инициализация пайплайна (детектор + распознаватель)\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "\n",
    "# Получение модели распознавания текста (можно дообучать)\n",
    "recognizer = pipeline.recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e9958f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer.model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',  # Для классификации символов\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46d9ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 88, in _transform\n        indices_grid = _meshgrid(output_height, output_width)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 62, in _meshgrid\n        x_linspace = tf.linspace(-1.0, 1.0, width)\n\n    ValueError: Exception encountered when calling layer 'lambda_9' (type Lambda).\n    \n    None values not supported.\n    \n    Call arguments received by layer 'lambda_9' (type Lambda):\n      • inputs=['tf.Tensor(shape=(None, None, None, 512), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)']\n      • mask=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mrecognizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Генератор из предыдущего шага\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecognizer_best.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensorBoard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filex1gsyygb.py:15\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(\u001b[38;5;28mself\u001b[39m), ag__.ld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     17\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py:88\u001b[39m, in \u001b[36m_transform\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m     86\u001b[39m output_height = output_size[\u001b[32m0\u001b[39m]\n\u001b[32m     87\u001b[39m output_width = output_size[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m indices_grid = \u001b[43m_meshgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m indices_grid = tf.expand_dims(indices_grid, \u001b[32m0\u001b[39m)\n\u001b[32m     90\u001b[39m indices_grid = tf.reshape(indices_grid, [-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# flatten?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py:62\u001b[39m, in \u001b[36m_meshgrid\u001b[39m\u001b[34m(height, width)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_meshgrid\u001b[39m(height, width):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     x_linspace = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     y_linspace = tf.linspace(-\u001b[32m1.0\u001b[39m, \u001b[32m1.0\u001b[39m, height)\n\u001b[32m     64\u001b[39m     x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
      "\u001b[31mValueError\u001b[39m: in user code:\n\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 88, in _transform\n        indices_grid = _meshgrid(output_height, output_width)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 62, in _meshgrid\n        x_linspace = tf.linspace(-1.0, 1.0, width)\n\n    ValueError: Exception encountered when calling layer 'lambda_9' (type Lambda).\n    \n    None values not supported.\n    \n    Call arguments received by layer 'lambda_9' (type Lambda):\n      • inputs=['tf.Tensor(shape=(None, None, None, 512), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)']\n      • mask=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "history = recognizer.model.fit(\n",
    "    train_gen,  # Генератор из предыдущего шага\n",
    "    steps_per_epoch=len(train_dataset) // batch_size,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=len(val_dataset) // batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint('recognizer_best.h5', save_best_only=True),\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
