{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f228ec",
   "metadata": {},
   "source": [
    "Чтобы использовать датасет, подготовленный для YOLO, для обучения keras-ocr, вам нужно преобразовать данные в формат, который понимает keras-ocr.\n",
    "\n",
    "1. Разница между форматами YOLO и Keras-OCR\n",
    "YOLO формат обычно хранит аннотации в .txt-файлах с координатами bounding box в формате:\n",
    "\n",
    "Copy\n",
    "class_id x_center y_center width height\n",
    "(нормализованные относительно ширины и высоты изображения).\n",
    "\n",
    "Keras-OCR ожидает данные в виде списка пар (image, [текст1, текст2, ...]), где bounding boxes могут быть в формате [[x1, y1], [x2, y2], [x3, y3], [x4, y4]] (координаты углов полигона).\n",
    "\n",
    "2. Преобразование YOLO → Keras-OCR\n",
    "\n",
    "# Шаг 1. Загрузка и парсинг YOLO-аннотаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58522fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 22:17:29.194712: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-22 22:17:29.985474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-22 22:17:29.985498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-22 22:17:30.030712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-22 22:17:30.139236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 22:17:31.010625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_ocr\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fadc9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_yolo_annotation(img_path, label_path):\n",
    "    # Загружаем изображение, чтобы узнать его размеры\n",
    "    img = cv2.imread(img_path)\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    \n",
    "    # Читаем аннотации из .txt-файла\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, w, h = map(float, line.strip().split())\n",
    "        \n",
    "        # Конвертация из нормализованных координат YOLO в абсолютные\n",
    "        x_center *= img_w\n",
    "        y_center *= img_h\n",
    "        w *= img_w\n",
    "        h *= img_h\n",
    "        \n",
    "        # Вычисление координат углов прямоугольника (x1, y1, x2, y2)\n",
    "        x1 = int(x_center - w / 2)\n",
    "        y1 = int(y_center - h / 2)\n",
    "        x2 = int(x_center + w / 2)\n",
    "        y2 = int(y_center + h / 2)\n",
    "        \n",
    "        # Добавляем в формате полигона (4 угла)\n",
    "        box = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]\n",
    "        boxes.append(box)\n",
    "    \n",
    "    return img, boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebe0a8",
   "metadata": {},
   "source": [
    "# Шаг 2. Создание датасета для Keras-OCR\n",
    "\n",
    "Keras-OCR ожидает данные в виде списка пар (изображение, список текстов). Если ваш датасет содержит только цифры, можно сгенерировать метки автоматически (предполагая, что class_id соответствует цифре):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1228d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_dataset(data_dir):\n",
    "    dataset = []\n",
    "    for img_file in os.listdir(os.path.join(data_dir, \"images\")):\n",
    "        if not img_file.endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(data_dir, \"images\", img_file)\n",
    "        label_path = os.path.join(data_dir, \"labels\", img_file.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        \n",
    "        img, boxes = parse_yolo_annotation(img_path, label_path)\n",
    "        \n",
    "        # Предполагаем, что class_id = цифра (0 → \"0\", 1 → \"1\" и т. д.)\n",
    "        texts = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_id = int(line.strip().split()[0])\n",
    "                texts.append(str(class_id))\n",
    "        \n",
    "        dataset.append((img, texts))  # Keras-OCR ожидает (image, [text1, text2, ...])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d2e84",
   "metadata": {},
   "source": [
    "# Шаг 3. Обучение Keras-OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "dataset = load_yolo_dataset(\"/home/lastinm/PROJECTS/DATA/syntetic digits\")\n",
    "\n",
    "train_dataset = dataset[:int(0.8 * len(dataset))]\n",
    "val_dataset = dataset[int(0.8 * len(dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0a2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_ocr_compatible_dataset(yolo_dataset):\n",
    "    \"\"\"Преобразует YOLO-датасет в формат, совместимый с keras-ocr\"\"\"\n",
    "    images = []\n",
    "    texts = []\n",
    "    boxes = []\n",
    "    \n",
    "    for img, annotations in yolo_dataset:\n",
    "        # Преобразование изображения\n",
    "        if len(img.shape) == 2:  # Чёрно-белое\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        else:  # Цветное\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_normalized = img_rgb.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Обработка аннотаций\n",
    "        line_texts = []\n",
    "        line_boxes = []\n",
    "        \n",
    "        if isinstance(annotations, str):  # Если аннотация - строка (класс)\n",
    "            line_texts.append(annotations)\n",
    "            line_boxes.append([[0, 0], [img.shape[1], 0], \n",
    "                             [img.shape[1], img.shape[0]], [0, img.shape[0]]])\n",
    "        elif isinstance(annotations, list):\n",
    "            for ann in annotations:\n",
    "                if isinstance(ann, str):  # Просто класс/текст\n",
    "                    line_texts.append(ann)\n",
    "                    line_boxes.append([[0, 0], [img.shape[1], 0], \n",
    "                                     [img.shape[1], img.shape[0]], [0, img.shape[0]]])\n",
    "                elif len(ann) >= 2:  # Кортеж/список (текст, bbox)\n",
    "                    line_texts.append(str(ann[0]))\n",
    "                    line_boxes.append(ann[1])\n",
    "                else:  # Неизвестный формат\n",
    "                    print(f\"Пропущена аннотация неизвестного формата: {ann}\")\n",
    "        \n",
    "        images.append(img_normalized)\n",
    "        texts.append(line_texts)\n",
    "        boxes.append(line_boxes)\n",
    "    \n",
    "    return images, texts, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac8900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый элемент train_dataset: (array([[[215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        ...,\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215]],\n",
      "\n",
      "       [[215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        ...,\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215]],\n",
      "\n",
      "       [[215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        ...,\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        ...,\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215]],\n",
      "\n",
      "       [[215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        ...,\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215]],\n",
      "\n",
      "       [[215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        ...,\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215],\n",
      "        [215, 215, 215]]], dtype=uint8), ['6', '0', '4'])\n",
      "Тип аннотаций: <class 'list'>\n",
      "Содержимое аннотаций: ['6', '0', '4']\n"
     ]
    }
   ],
   "source": [
    "print(\"Первый элемент train_dataset:\", train_dataset[0])\n",
    "print(\"Тип аннотаций:\", type(train_dataset[0][1]))\n",
    "print(\"Содержимое аннотаций:\", train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22575011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример преобразованных данных:\n",
      "Текст: ['6', '0', '4']\n",
      "Боксы: [[[0, 0], [640, 0], [640, 640], [0, 640]], [[0, 0], [640, 0], [640, 640], [0, 640]], [[0, 0], [640, 0], [640, 640], [0, 640]]]\n"
     ]
    }
   ],
   "source": [
    "train_images, train_texts, train_boxes = build_keras_ocr_compatible_dataset(train_dataset)\n",
    "print(\"Пример преобразованных данных:\")\n",
    "print(\"Текст:\", train_texts[0])\n",
    "print(\"Боксы:\", train_boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9c4cdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 88, in _transform\n        indices_grid = _meshgrid(output_height, output_width)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 62, in _meshgrid\n        x_linspace = tf.linspace(-1.0, 1.0, width)\n\n    ValueError: Exception encountered when calling layer 'lambda_17' (type Lambda).\n    \n    None values not supported.\n    \n    Call arguments received by layer 'lambda_17' (type Lambda):\n      • inputs=['tf.Tensor(shape=(None, None, None, 512), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)']\n      • mask=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m val_gen = data_generator(val_dataset, batch_size)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m history = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filejdzf85ix.py:15\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(\u001b[38;5;28mself\u001b[39m), ag__.ld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     17\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py:88\u001b[39m, in \u001b[36m_transform\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m     86\u001b[39m output_height = output_size[\u001b[32m0\u001b[39m]\n\u001b[32m     87\u001b[39m output_width = output_size[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m indices_grid = \u001b[43m_meshgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m indices_grid = tf.expand_dims(indices_grid, \u001b[32m0\u001b[39m)\n\u001b[32m     90\u001b[39m indices_grid = tf.reshape(indices_grid, [-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# flatten?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py:62\u001b[39m, in \u001b[36m_meshgrid\u001b[39m\u001b[34m(height, width)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_meshgrid\u001b[39m(height, width):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     x_linspace = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     y_linspace = tf.linspace(-\u001b[32m1.0\u001b[39m, \u001b[32m1.0\u001b[39m, height)\n\u001b[32m     64\u001b[39m     x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
      "\u001b[31mValueError\u001b[39m: in user code:\n\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 88, in _transform\n        indices_grid = _meshgrid(output_height, output_width)\n    File \"/home/lastinm/PROJECTS/credit_cards_detection/.venv/lib/python3.11/site-packages/keras_ocr/recognition.py\", line 62, in _meshgrid\n        x_linspace = tf.linspace(-1.0, 1.0, width)\n\n    ValueError: Exception encountered when calling layer 'lambda_17' (type Lambda).\n    \n    None values not supported.\n    \n    Call arguments received by layer 'lambda_17' (type Lambda):\n      • inputs=['tf.Tensor(shape=(None, None, None, 512), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)']\n      • mask=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# Создание генераторов\n",
    "batch_size = 16\n",
    "train_gen = data_generator(train_dataset, batch_size)\n",
    "val_gen = data_generator(val_dataset, batch_size)\n",
    "\n",
    "# Обучение модели\n",
    "history = pipeline.recognizer.model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_dataset) // batch_size,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=len(val_dataset) // batch_size,\n",
    "    epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
